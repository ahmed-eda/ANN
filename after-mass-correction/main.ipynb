{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN for make simulation and make prediction in physics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "source": [
    "# import working liberary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variable to program\n",
    "#inputFile = 'datasets/collect data_pi-modified.xlsx'\n",
    "inputFile = 'All data mesons+baryons.xlsx'\n",
    "inputSheetName = 'Sheet1'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the excel file\n",
    "data_all = pd.read_excel(inputFile,sheet_name=inputSheetName)\n",
    "temp_data_all = data_all[data_all['spectrum']<60]\n",
    "data =temp_data_all.reset_index(drop=True)\n",
    "\n",
    "# Split the data into input and output variables\n",
    "X = data[['mass','s','N part','Pt']]\n",
    "y = data['spectrum'].to_frame('spectrum')\n",
    "\n",
    "print(X.head)\n",
    "print(y.head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normaliz input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# Create a RobustScaler object\n",
    "scaler = RobustScaler()\n",
    "# Fit the scaler to the input data and transform it\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "# Print the normalized input data\n",
    "print('X_normalized')\n",
    "print(X_normalized)\n",
    "X_train = X_normalized\n",
    "#X_train = X\n",
    "print('X_train')\n",
    "print(X_train) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loada saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "modelName = 'correction_mass_test-11-6-2023-8L-100-16.h5'\n",
    "#model = load_model(modelName)\n",
    "#configure output parameters\n",
    "outputFile = 'out_in4- '+modelName+' .xlsx'\n",
    "summaryOutFile = modelName + ' - Summary .txt'\n",
    "outputSheetName = 'predicat_in4-good -  '+modelName+' '\n",
    "nameFigImg = 'fig_in4- '+modelName+' .png'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the model - compile - fit - save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Define the model '''\n",
    "# Define the model\n",
    "model = Sequential(name=modelName)\n",
    "# Add the first dense layer\n",
    "model.add(Dense(40, input_dim=4, activation='relu'))\n",
    "\n",
    "# Add batch normalization\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(40, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "''' # compile the model      '''\n",
    "# Compile the model with Levenberg-Marquardt optimizer\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.001,)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "''' train the model & save current compiled model  '''\n",
    "# Train the model\n",
    "#model.fit(X, y, epochs=100, batch_size=32, validation_split=0.2)\n",
    "model.fit(X_train, y, epochs=100, batch_size=16) \n",
    "# Save the model\n",
    "model.save('correction_mass_test-11-6-2023-8L-100-16.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model and make prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on new data\n",
    "X_test =pd.DataFrame(X_train) #scaler.transform(X)\n",
    "#X_test = scaler.fit_transform(X)\n",
    "print(\"new_data is : \")\n",
    "print(X_test)\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.flatten()\n",
    "predictions = pd.Series(predictions)\n",
    "predictions = predictions.to_frame('predictions')\n",
    "print(\"predictions is : \")\n",
    "print(predictions)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y)\n",
    "print(\"score \" , score)\n",
    "print(score)\n",
    "mse = mean_squared_error(y,predictions)\n",
    "print('mse' , mse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for drawing in 2d i choose Pt as x-axis\n",
    "error = data['spectrum'] - predictions['predictions']\n",
    "error = error.to_frame('error')\n",
    "\n",
    "datap = pd.merge(data,predictions,left_index=True, right_index=True)\n",
    "print('shape of datap',datap.shape)\n",
    "\n",
    "print('data : \\n',data)\n",
    "print('pred \\n ',predictions)\n",
    "print('datap \\n',datap)\n",
    "\n",
    "# xap : data\n",
    "xap = pd.DataFrame(datap)\n",
    "# xapf : xap after filteration\n",
    "xapf= pd.DataFrame(datap)\n",
    "xapf = xapf[xapf['mass']==139.57]\n",
    "xapf = xapf[xapf['s']==7.7]\n",
    "#xapf = xapf[xapf['N part']==337]\n",
    "N_Part_Values  =  xapf['N part'].unique()\n",
    "print('Npart values : \\n')\n",
    "for n in N_Part_Values:\n",
    "    print('N is : ',n)\n",
    "print('Npart values : \\n',N_Part_Values)\n",
    "dataGraph1 = pd.merge(xapf['Pt'],xapf['predictions'],left_index=True, right_index=True)\n",
    "print('datagraph1 : \\n',dataGraph1)\n",
    "dataGraph = pd.merge(dataGraph1,xapf['spectrum'],left_index=True, right_index=True)\n",
    "print('dataGraph : \\n', dataGraph)\n",
    "print('shape of dataGraph',dataGraph.shape)\n",
    "# Plot the data and predictions\n",
    "mergedData = pd.merge(dataGraph,xapf['N part'],left_index=True, right_index=True)\n",
    "print('merged data is : \\n',mergedData)\n",
    "#plt.semilogy(xapf['Pt'], xapf['Spectrum']   ,'bo', label='Actual')\n",
    "#plt.semilogy(xapf['Pt'], xapf['predictions'],'ro', label='Predicted')\n",
    "for n in N_Part_Values:\n",
    "    plt.scatter(mergedData['Pt'][mergedData['N part']==n],mergedData['spectrum'][mergedData['N part']==n])\n",
    "    plt.scatter(mergedData['Pt'][mergedData['N part']==n],mergedData['predictions'][mergedData['N part']==n])\n",
    "    \n",
    "   ##########\n",
    "import matplotlib.pyplot as plt\n",
    "# Define the list ofValues and plot the data for each iteration\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(nrows=len(N_Part_Values), ncols=1, figsize=(10, 50))\n",
    "\n",
    "#fig, axs = plt.subplots(N_Part_Values.size,1)\n",
    "for i, n in enumerate(N_Part_Values):\n",
    "    # Plot the 'Pt' column where N_part == n\n",
    "    axs[i].scatter(mergedData['Pt'][mergedData['N part'] == n], \n",
    "                 mergedData['spectrum'][mergedData['N part'] == n], \n",
    "                 color='C{}'.format(i), \n",
    "                 label='N_part = {}'.format(n))\n",
    "\n",
    "    # Plot the 'predictions' column where N_part == n\n",
    "    axs[i].scatter(mergedData['Pt'][mergedData['N part'] == n], \n",
    "                 mergedData['predictions'][mergedData['N part'] == n], \n",
    "                 color='black', \n",
    "                  label=' predictions N_part = {}'.format(n))\n",
    "                 #label='_nolegend_')\n",
    "\n",
    "    # Add a legend and axis labels to the subplot\n",
    "    axs[i].legend()\n",
    "    axs[i].set_xlabel('Pt')\n",
    "    axs[i].set_ylabel('Value')\n",
    "    axs[i].set_title('N_part = {}'.format(n))\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "##########\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write output to excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions , data to Excel file\n",
    "err1=data['err1'].to_frame('err1')\n",
    "err2=data['err2'].to_frame('err2')\n",
    "#SquareErrorForEachPoint = np.sqrt( ((datap['predictions']- datap['Spectrum'])/(err1- err2)))\n",
    "SquareErrorForEachPoint =np.square( (datap['predictions']- datap['spectrum'])/(err1['err1']+ err2['err2']))\n",
    "\n",
    "SquareErrorForEachPoint = pd.Series(SquareErrorForEachPoint)\n",
    "SquareErrorForEachPoint = SquareErrorForEachPoint.to_frame('SquareErrorForEachPoint')\n",
    "print('Square error for each point : ',SquareErrorForEachPoint)\n",
    "\n",
    "outputpredicat = pd.concat([datap, SquareErrorForEachPoint], axis=1)\n",
    "mysum =outputpredicat['SquareErrorForEachPoint'].sum()\n",
    "mycount =(outputpredicat['SquareErrorForEachPoint'].count()) -1\n",
    "rmse = np.sqrt(mysum/mycount)\n",
    "\n",
    "#rmse = np.sqrt (np.average(outputpredicat['SquareErrorForEachPoint']))\n",
    "rmse = pd.Series(rmse)\n",
    "#rmse = pd.DataFrame({'rmse': rmse})\n",
    "print('RMSE',rmse)\n",
    "\n",
    "# output is data frame\n",
    "#print(outputpredicat.head(10))\n",
    "# Write the DataFrames to an Excel file with three sheets\n",
    "with pd.ExcelWriter(outputFile) as writer:\n",
    "    outputpredicat.to_excel(writer, sheet_name=outputSheetName, index=False)\n",
    "    rmse.to_excel(writer, sheet_name='RMSE', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print model summery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"End\")\n",
    "#print(\"accuracy :\" + str(accuracy))\n",
    "print(\"score \" + str(score))\n",
    "SummaryOut = model.summary()\n",
    "print(SummaryOut)\n",
    "\n",
    "from io import StringIO\n",
    "# summarize the model\n",
    "with StringIO() as buf:\n",
    "    model.summary(print_fn=lambda x: buf.write(x + '\\n'))\n",
    "    summary = buf.getvalue()\n",
    "\n",
    "with open(modelName +'-summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "\n",
    "print(summary)\n",
    "print(modelName)\n",
    "print('RMSE',rmse)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
