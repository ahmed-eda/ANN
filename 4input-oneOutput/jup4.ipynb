{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN for make simulation and make prediction in physics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "source": [
    "# import working liberary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variable to program\n",
    "#inputFile = 'datasets/collect data_pi-modified.xlsx'\n",
    "inputFile = 'All data mesons+baryons.xlsx'\n",
    "inputSheetName = 'main'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         mass     s  N part     Pt\n",
      "3     139.57   7.7     337  0.425\n",
      "4     139.57   7.7     337  0.475\n",
      "5     139.57   7.7     337  0.525\n",
      "6     139.57   7.7     337  0.575\n",
      "7     139.57   7.7     337  0.625\n",
      "...      ...   ...     ...    ...\n",
      "6559  938.27  39.0      14  1.550\n",
      "6560  938.27  39.0      14  1.650\n",
      "6561  938.27  39.0      14  1.750\n",
      "6562  938.27  39.0      14  1.850\n",
      "6563  938.27  39.0      14  1.950\n",
      "\n",
      "[6450 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "\"\"\" from openpyxl import Workbook\n",
    "import openpyxl \"\"\"\n",
    "# Read the data from the excel file\n",
    "data_all = pd.read_excel(inputFile,sheet_name=inputSheetName)\n",
    "data = data_all[data_all['Spectrum']<60]\n",
    "#data = data_all\n",
    "#data = data[data['N part']==337]\n",
    "# Split the data into input and output variables\n",
    "X = data[['mass','s','N part','Pt']]\n",
    "y = data['Spectrum'].to_frame('Spectrum')\n",
    "\n",
    "print(X.head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normaliz input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_normalized\n",
      "[[-0.4433542  -0.76774194  1.22702703 -0.40625   ]\n",
      " [-0.4433542  -0.76774194  1.22702703 -0.34375   ]\n",
      " [-0.4433542  -0.76774194  1.22702703 -0.28125   ]\n",
      " ...\n",
      " [ 0.5566458   1.2516129  -0.51891892  1.25      ]\n",
      " [ 0.5566458   1.2516129  -0.51891892  1.375     ]\n",
      " [ 0.5566458   1.2516129  -0.51891892  1.5       ]]\n",
      "X_train\n",
      "[[-0.4433542  -0.76774194  1.22702703 -0.40625   ]\n",
      " [-0.4433542  -0.76774194  1.22702703 -0.34375   ]\n",
      " [-0.4433542  -0.76774194  1.22702703 -0.28125   ]\n",
      " ...\n",
      " [ 0.5566458   1.2516129  -0.51891892  1.25      ]\n",
      " [ 0.5566458   1.2516129  -0.51891892  1.375     ]\n",
      " [ 0.5566458   1.2516129  -0.51891892  1.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the input\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a RobustScaler object\n",
    "scaler = RobustScaler()\n",
    "#scaler = StandardScaler()\n",
    "# Fit the scaler to the input data and transform it\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "# Print the normalized input data\n",
    "print('X_normalized')\n",
    "print(X_normalized)\n",
    "X_train = X_normalized\n",
    "#X_train = X\n",
    "print('X_train')\n",
    "print(X_train) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loada saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "modelName = 'good - all_hadrons_4_test-19-5-2023-8L-100-16.h5'\n",
    "model = load_model(modelName)\n",
    "#configure output parameters\n",
    "outputFile = 'out_in4- '+modelName+' .xlsx'\n",
    "summaryOutFile = modelName + ' - Summary .txt'\n",
    "outputSheetName = 'predicat_in4-good -  '+modelName+' '\n",
    "nameFigImg = 'fig_in4- '+modelName+' .png'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the model - compile - fit - save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Define the model\\nmodelName = 'all-hadron-4in.h5'\\nmodel = Sequential(name=modelName)\\n# Add the first dense layer\\nmodel.add(Dense(40, input_dim=4, activation='relu'))\\n\\n# Add batch normalization\\n#model.add(BatchNormalization())\\nmodel.add(Dense(40, activation='relu'))\\n#model.add(BatchNormalization())\\nmodel.add(Dense(80, activation='relu'))\\nmodel.add(Dense(80, activation='relu'))\\nmodel.add(Dense(40, activation='relu'))\\nmodel.add(Dense(40, activation='relu'))\\n\\n# Add the output layer\\nmodel.add(Dense(1))\\n# Add the output layer\\n#model.add(Dense(1, activation='softmax')) \\n\\n''' # compile the model      '''\\n\\n# Compile the model with Levenberg-Marquardt optimizer\\noptimizer = RMSprop(learning_rate=0.001, rho=0.001,)\\nmodel.compile(loss='mean_squared_error', optimizer=optimizer)\\n\\n''' train the model & save current compiled model  '''\\n# Train the model\\n#model.fit(X, y, epochs=100, batch_size=32, validation_split=0.2)\\nmodel.fit(X_train, y, epochs=100, batch_size=16) \\n# Save the model\\nmodel.save('modified-input-all_hadrons_4_test-19-5-2023-8L-100-16.h5')\\n\\n\\n \""
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "''' Define the model '''\n",
    "\"\"\" # Define the model\n",
    "modelName = 'all-hadron-4in.h5'\n",
    "model = Sequential(name=modelName)\n",
    "# Add the first dense layer\n",
    "model.add(Dense(40, input_dim=4, activation='relu'))\n",
    "\n",
    "# Add batch normalization\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(40, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n",
    "# Add the output layer\n",
    "#model.add(Dense(1, activation='softmax')) \n",
    "\n",
    "''' # compile the model      '''\n",
    "\n",
    "# Compile the model with Levenberg-Marquardt optimizer\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.001,)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "''' train the model & save current compiled model  '''\n",
    "# Train the model\n",
    "#model.fit(X, y, epochs=100, batch_size=32, validation_split=0.2)\n",
    "model.fit(X_train, y, epochs=100, batch_size=16) \n",
    "# Save the model\n",
    "model.save('modified-input-all_hadrons_4_test-19-5-2023-8L-100-16.h5')\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model and make prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_data is : \n",
      "             0         1         2        3\n",
      "0    -0.443354 -0.767742  1.227027 -0.40625\n",
      "1    -0.443354 -0.767742  1.227027 -0.34375\n",
      "2    -0.443354 -0.767742  1.227027 -0.28125\n",
      "3    -0.443354 -0.767742  1.227027 -0.21875\n",
      "4    -0.443354 -0.767742  1.227027 -0.15625\n",
      "...        ...       ...       ...      ...\n",
      "6445  0.556646  1.251613 -0.518919  1.00000\n",
      "6446  0.556646  1.251613 -0.518919  1.12500\n",
      "6447  0.556646  1.251613 -0.518919  1.25000\n",
      "6448  0.556646  1.251613 -0.518919  1.37500\n",
      "6449  0.556646  1.251613 -0.518919  1.50000\n",
      "\n",
      "[6450 rows x 4 columns]\n",
      "202/202 [==============================] - 0s 1ms/step\n",
      "predictions is : \n",
      "      predictions\n",
      "0       50.695030\n",
      "1       39.451057\n",
      "2       30.011566\n",
      "3       22.124079\n",
      "4       16.366070\n",
      "...           ...\n",
      "6445    -0.013973\n",
      "6446     0.001270\n",
      "6447     0.020193\n",
      "6448     0.020156\n",
      "6449     0.009678\n",
      "\n",
      "[6450 rows x 1 columns]\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 2.2244\n",
      "score  2.2243964672088623\n",
      "2.2243964672088623\n",
      "mse 2.2243966097732755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on new data\n",
    "X_test =pd.DataFrame(X_train) #scaler.transform(X)\n",
    "#X_test = scaler.fit_transform(X)\n",
    "print(\"new_data is : \")\n",
    "print(X_test)\n",
    "predictions = model.predict(X_test)\n",
    "predictions = predictions.flatten()\n",
    "predictions = pd.Series(predictions)\n",
    "predictions = predictions.to_frame('predictions')\n",
    "print(\"predictions is : \")\n",
    "print(predictions)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y)\n",
    "print(\"score \" , score)\n",
    "print(score)\n",
    "mse = mean_squared_error(y,predictions)\n",
    "print('mse' , mse)\n",
    "\n",
    "#print(\"accuracy\")\n",
    "#print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data (6450, 7)\n",
      "shape of pred (6450, 1)\n",
      "shape of datap (0, 8)\n",
      "data : \n",
      "         mass     s  N part     Pt  Spectrum     err1     err2\n",
      "3     139.57   7.7     337  0.425   46.3708  0.11247  3.01883\n",
      "4     139.57   7.7     337  0.475   34.7823  0.09267  2.24990\n",
      "5     139.57   7.7     337  0.525   26.3221  0.07704  1.69598\n",
      "6     139.57   7.7     337  0.575   19.7093  0.06396  1.26704\n",
      "7     139.57   7.7     337  0.625   15.0548  0.05379  0.96702\n",
      "...      ...   ...     ...    ...       ...      ...      ...\n",
      "6559  938.27  39.0      14  1.550    0.0073  0.00008  0.00070\n",
      "6560  938.27  39.0      14  1.650    0.0049  0.00007  0.00057\n",
      "6561  938.27  39.0      14  1.750    0.0034  0.00005  0.00039\n",
      "6562  938.27  39.0      14  1.850    0.0023  0.00004  0.00026\n",
      "6563  938.27  39.0      14  1.950    0.0016  0.00003  0.00018\n",
      "\n",
      "[6450 rows x 7 columns]\n",
      "pred \n",
      "        predictions\n",
      "0       50.695030\n",
      "1       39.451057\n",
      "2       30.011566\n",
      "3       22.124079\n",
      "4       16.366070\n",
      "...           ...\n",
      "6445    -0.013973\n",
      "6446     0.001270\n",
      "6447     0.020193\n",
      "6448     0.020156\n",
      "6449     0.009678\n",
      "\n",
      "[6450 rows x 1 columns]\n",
      "datap \n",
      " Empty DataFrame\n",
      "Columns: [mass, s, N part, Pt, Spectrum, err1, err2, predictions]\n",
      "Index: []\n",
      "datap shap : \n",
      " (0, 8)\n",
      "xapf : \n",
      " Empty DataFrame\n",
      "Columns: [mass, s, N part, Pt, Spectrum, err1, err2, predictions]\n",
      "Index: []\n",
      "xapf shap : \n",
      " (0, 8)\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[197], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mxapf : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,xapf)\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mxapf shap : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,xapf\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 24\u001b[0m dataGraph1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(xapf[\u001b[39m'\u001b[39;49m\u001b[39mPt\u001b[39;49m\u001b[39m'\u001b[39;49m],xapf[\u001b[39m'\u001b[39;49m\u001b[39mpredictions\u001b[39;49m\u001b[39m'\u001b[39;49m],how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minner\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdatagraph1 : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,dataGraph1)\n\u001b[0;32m     26\u001b[0m dataGraph \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(dataGraph1,xapf[\u001b[39m'\u001b[39m\u001b[39mSpectrum\u001b[39m\u001b[39m'\u001b[39m],how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minner\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Repos\\ReposEda\\DrSamahPrjs\\ANN\\ANN\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:142\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 142\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    143\u001b[0m         left,\n\u001b[0;32m    144\u001b[0m         right,\n\u001b[0;32m    145\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    146\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    147\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    148\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    149\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    150\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    151\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    152\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    153\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    154\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32md:\\Repos\\ReposEda\\DrSamahPrjs\\ANN\\ANN\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:713\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    706\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    707\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNot allowed to merge between different levels. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m_left\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels\u001b[39m}\u001b[39;00m\u001b[39m levels on the left, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_right\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels\u001b[39m}\u001b[39;00m\u001b[39m on the right)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    710\u001b[0m     )\n\u001b[0;32m    711\u001b[0m     \u001b[39mraise\u001b[39;00m MergeError(msg)\n\u001b[1;32m--> 713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_on, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_on \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_left_right_on(left_on, right_on)\n\u001b[0;32m    715\u001b[0m cross_col \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhow \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcross\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Repos\\ReposEda\\DrSamahPrjs\\ANN\\ANN\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1480\u001b[0m, in \u001b[0;36m_MergeOperation._validate_left_right_on\u001b[1;34m(self, left_on, right_on)\u001b[0m\n\u001b[0;32m   1478\u001b[0m common_cols \u001b[39m=\u001b[39m left_cols\u001b[39m.\u001b[39mintersection(right_cols)\n\u001b[0;32m   1479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(common_cols) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mraise\u001b[39;00m MergeError(\n\u001b[0;32m   1481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo common columns to perform merge on. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1482\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMerge options: left_on=\u001b[39m\u001b[39m{\u001b[39;00mleft_on\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1483\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mright_on=\u001b[39m\u001b[39m{\u001b[39;00mright_on\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1484\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft_index=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_index\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1485\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mright_index=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_index\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1486\u001b[0m     )\n\u001b[0;32m   1487\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     \u001b[39mnot\u001b[39;00m left_cols\u001b[39m.\u001b[39mjoin(common_cols, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mis_unique\n\u001b[0;32m   1489\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m right_cols\u001b[39m.\u001b[39mjoin(common_cols, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mis_unique\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[0;32m   1491\u001b[0m     \u001b[39mraise\u001b[39;00m MergeError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData columns not unique: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(common_cols)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mMergeError\u001b[0m: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False"
     ]
    }
   ],
   "source": [
    "# for drawing in 2d i choose Pt as x-axis\n",
    "error = data['Spectrum'] - predictions['predictions']\n",
    "error = error.to_frame('error')\n",
    "print('shape of data' , data.shape)\n",
    "print('shape of pred' , predictions.shape)\n",
    "#datap = pd.concat([data,predictions],axis=1)\n",
    "datap = pd.merge(data,predictions,left_on='Pt',right_on='predictions')\n",
    "print('shape of datap',datap.shape)\n",
    "\n",
    "print('data : \\n',data)\n",
    "print('pred \\n ',predictions)\n",
    "print('datap \\n',datap)\n",
    "\n",
    "# xap : data\n",
    "xap = pd.DataFrame(datap)\n",
    "# xapf : xap after filteration\n",
    "xapf= pd.DataFrame(datap)\n",
    "xapf = xapf[xapf['mass']==139.57]\n",
    "xapf = xapf[xapf['s']==7.7]\n",
    "xapf = xapf[xapf['N part']==337]\n",
    "\n",
    "print ('xapf : \\n',xapf)\n",
    "print('datap shap : \\n',datap.shape)\n",
    "print ('xapf shap : \\n',xapf.shape)\n",
    "dataGraph1 = pd.merge(xapf['Pt'],xapf['predictions'],how='inner')\n",
    "print('datagraph1 : \\n',dataGraph1)\n",
    "dataGraph = pd.merge(dataGraph1,xapf['Spectrum'],how='inner')\n",
    "print('dataGraph : \\n', dataGraph)\n",
    "#dataGraph = pd.concat([xapf['Pt'],xapf['predictions'],xapf['Spectrum']],axis=1)\n",
    "print(dataGraph)\n",
    "print('shape of dataGraph',dataGraph.shape)\n",
    "print('shape of dataGraph',dataGraph.shape)\n",
    "\n",
    "# Plot the data and predictions\n",
    "\n",
    "#plt.semilogy(xapf['Pt'], xapf['Spectrum']   ,'bo', label='Actual')\n",
    "#plt.semilogy(xapf['Pt'], xapf['predictions'],'ro', label='Predicted')\n",
    "\"\"\" plt.scatter(xapf['Pt'], xapf['Spectrum'])\n",
    "plt.scatter(xapf['Pt'], xapf['predictions'])\n",
    "plt.xlabel('Pt')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Ac'])\n",
    "plt.show()\n",
    "plt.savefig(nameFigImg) \"\"\"\n",
    "\n",
    "plt.scatter(dataGraph['Pt'], dataGraph['Spectrum'])\n",
    "plt.scatter(dataGraph['Pt'], dataGraph['predictions'])\n",
    "plt.xlabel('Pt')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Actual Spectrum', 'Predicted Spectrum'])\n",
    "plt.show()\n",
    "plt.savefig(nameFigImg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write output to excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write predictions , data to Excel file\n",
    "err1=data['err1'].to_frame('err1')\n",
    "err2=data['err2'].to_frame('err2')\n",
    "#SquareErrorForEachPoint = np.sqrt( ((datap['predictions']- datap['Spectrum'])/(err1- err2)))\n",
    "SquareErrorForEachPoint =np.square( (datap['predictions']- datap['Spectrum'])/(err1['err1']+ err2['err2']))\n",
    "\n",
    "SquareErrorForEachPoint = pd.Series(SquareErrorForEachPoint)\n",
    "SquareErrorForEachPoint = SquareErrorForEachPoint.to_frame('SquareErrorForEachPoint')\n",
    "print('Square error for each point : ',SquareErrorForEachPoint)\n",
    "\n",
    "outputpredicat = pd.concat([datap, SquareErrorForEachPoint], axis=1)\n",
    "#pd.DataFrame([datap,SquareErrorForEachPoint])\n",
    "# Calculate the RMSE.\n",
    "#rmse = mean_squared_error(outputpredicat['Spectrum'], outputpredicat['predictions'])\n",
    "mysum =outputpredicat['SquareErrorForEachPoint'].sum()\n",
    "mycount =(outputpredicat['SquareErrorForEachPoint'].count()) -1\n",
    "rmse = np.sqrt(mysum/mycount)\n",
    "\n",
    "#rmse = np.sqrt (np.average(outputpredicat['SquareErrorForEachPoint']))\n",
    "rmse = pd.Series(rmse)\n",
    "#rmse = pd.DataFrame({'rmse': rmse})\n",
    "print('RMSE',rmse)\n",
    "\n",
    "# output is data frame\n",
    "#print(outputpredicat.head(10))\n",
    "# Write the DataFrames to an Excel file with three sheets\n",
    "with pd.ExcelWriter(outputFile) as writer:\n",
    "    outputpredicat.to_excel(writer, sheet_name=outputSheetName, index=False)\n",
    "    rmse.to_excel(writer, sheet_name='RMSE', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print model summery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"End\")\n",
    "#print(\"accuracy :\" + str(accuracy))\n",
    "print(\"score \" + str(score))\n",
    "SummaryOut = model.summary()\n",
    "print(SummaryOut)\n",
    "\n",
    "from io import StringIO\n",
    "# summarize the model\n",
    "with StringIO() as buf:\n",
    "    model.summary(print_fn=lambda x: buf.write(x + '\\n'))\n",
    "    summary = buf.getvalue()\n",
    "\n",
    "with open(modelName +'-summary.txt', 'w') as f:\n",
    "    f.write(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
